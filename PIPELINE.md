# Pipeline

The export pipeline bridges Sieve (private intelligence engine) and brengel.com (public site). It reads from Sieve's SQLite database, generates markdown content files, and stages them for review before publishing.

## Principles

1. **Sieve proposes, you dispose.** The pipeline generates content. You decide what goes live. Nothing auto-publishes.
2. **Idempotent.** Running the export twice for the same date produces the same output. No side effects, no accumulated state.
3. **One-way.** Data flows from Sieve → site. Never the reverse. The site repo never writes back to Sieve's database.
4. **Fail loudly.** If the database schema changed, if a required column is missing, if the digest is empty — the script fails with a clear error, not a silent empty file.

## Pipeline Script: `export.py`

### Usage

```bash
# Export today's digest and articles
python scripts/export.py --sieve-db /path/to/sieve.db --output ./content

# Export a specific date
python scripts/export.py --sieve-db /path/to/sieve.db --output ./content --date 2026-02-15

# Export a date range (backfill)
python scripts/export.py --sieve-db /path/to/sieve.db --output ./content \
    --from 2026-01-01 --to 2026-02-15

# Dry run — show what would be generated without writing files
python scripts/export.py --sieve-db /path/to/sieve.db --output ./content --dry-run

# Export only the article index (for Reader page)
python scripts/export.py --sieve-db /path/to/sieve.db --output ./content --articles-only

# Export only digests (for landing page)
python scripts/export.py --sieve-db /path/to/sieve.db --output ./content --digests-only
```

### What It Produces

```
content/
├── digests/
│   ├── 2026-02-15.md           # Full digest with YAML frontmatter
│   ├── 2026-02-14.md
│   └── ...
├── articles/
│   └── index.json              # Complete article index for Reader
└── threads/
    └── index.json              # Active thread metadata
```

### Digest Export

For each date, the script:

1. Queries `digests` table for the given date
2. Queries `articles` table for all articles scored on that date
3. Queries `threads` table for any threads active on that date
4. Generates a markdown file with YAML frontmatter

```yaml
---
date: "2026-02-15"
title: "The Signal — February 15, 2026"
summary: >
  The big picture paragraph extracted from the digest.
  This appears on the landing page card.
article_count: 47
source_count: 23
top_threads:
  - label: "regulatory capture AI safety boards"
    convergence: 0.87
    article_count: 6
  - label: "semiconductor export controls"
    convergence: 0.72
    article_count: 4
top_scoring_articles:
  - title: "Article Title"
    source: "Publication"
    score: 9.1
    url: "https://..."
  - title: "Another Article"
    source: "Publication"
    score: 8.7
    url: "https://..."
---

[Full digest content in markdown, as generated by Sieve]
```

**Summary extraction:** The summary field is the first paragraph of the digest (or a configured "summary" field if Sieve stores one separately). This is the text that appears on the landing page card. It should be self-contained — readable without clicking through.

### Article Index Export

The article index is a single JSON file containing metadata for every article in Sieve's database. This file powers the Reader page's client-side filtering and sorting.

```json
{
  "generated_at": "2026-02-15T12:00:00Z",
  "total_articles": 12847,
  "articles": [
    {
      "id": 1234,
      "title": "Article Title as Published",
      "source": "Publication Name",
      "url": "https://original-url.com/article",
      "published": "2026-02-15",
      "ingested": "2026-02-15T08:30:00Z",
      "overall_score": 7.8,
      "axiom_scores": {
        "axiom_name_1": 8.0,
        "axiom_name_2": 7.5
      },
      "threads": ["thread-slug-1", "thread-slug-2"],
      "digest_date": "2026-02-15",
      "in_digest": true
    }
  ]
}
```

**Size considerations:** With 300+ articles per day, the index will grow to tens of thousands of entries. At ~200 bytes per entry compressed, 10,000 articles ≈ 2MB gzipped. Options if this becomes too large:

- Paginate the JSON: `articles-2026-02.json`, `articles-2026-01.json`, etc.
- Serve only the most recent N months, with an archive search option
- Use a SQLite database served as a static file with [sql.js](https://sql.js.org/) for client-side querying

Start with the single JSON file. Optimize when the data says you need to.

### Thread Index Export

```json
{
  "generated_at": "2026-02-15T12:00:00Z",
  "active_threads": [
    {
      "id": "thread-slug",
      "label": "Regulatory Capture of AI Safety Boards",
      "first_seen": "2026-02-03",
      "last_seen": "2026-02-15",
      "article_count": 14,
      "convergence": 0.87,
      "article_ids": [1234, 1256, 1278, 1301]
    }
  ]
}
```

## Workflow

### Daily Flow

```
Morning:
  1. Sieve runs its daily pipeline (RSS → score → digest)
  2. Review the digest in Sieve's local interface
  
When ready to publish:
  3. Run export.py for today's date
  4. Review generated markdown in content/digests/
  5. Edit if needed (fix typos, adjust summary, etc.)
  6. git add, git commit
  7. Build site: hugo (or zola build)
  8. Deploy: rsync public/ to server

Periodically:
  9. Run export.py --articles-only to update the Reader index
  10. Rebuild and deploy
```

### Backfill Flow

When setting up the site for the first time or adding historical content:

```bash
# Export all available digests
python scripts/export.py --sieve-db /path/to/sieve.db --output ./content \
    --from 2025-01-01 --to 2026-02-15

# Review the batch
ls content/digests/

# Build and deploy
hugo && rsync -avz --delete public/ user@server:/var/www/brengel.com/
```

### The Review Step

This is the most important part of the pipeline. Before any content goes live:

- **Read the summary.** Does it accurately capture the day's signal? Would a stranger understand it?
- **Read the full digest.** Is Abend's voice consistent? Are the article references accurate?
- **Check the article index.** Are scores reasonable? Any obvious scoring errors?
- **Check thread labels.** Do the detected threads make sense? Are any mislabeled?

The review is not quality control in a corporate sense. It's the point where private intelligence becomes public communication. Sieve sees what it sees. You decide what's worth sharing.

## Script Architecture

```python
# scripts/export.py — pseudocode structure

class SieveExporter:
    def __init__(self, db_path: str, output_dir: str):
        self.db = sqlite3.connect(db_path)
        self.output = Path(output_dir)
    
    def export_digest(self, date: str) -> Path:
        """Export a single day's digest as markdown with frontmatter."""
        digest = self.query_digest(date)
        articles = self.query_articles_for_date(date)
        threads = self.query_active_threads(date)
        
        if not digest:
            raise ExportError(f"No digest found for {date}")
        
        frontmatter = self.build_frontmatter(digest, articles, threads)
        content = digest['full_content']
        
        path = self.output / 'digests' / f'{date}.md'
        path.write_text(f'---\n{yaml.dump(frontmatter)}---\n\n{content}')
        return path
    
    def export_article_index(self) -> Path:
        """Export complete article index as JSON for the Reader."""
        articles = self.query_all_articles()
        
        index = {
            'generated_at': datetime.utcnow().isoformat(),
            'total_articles': len(articles),
            'articles': [self.format_article(a) for a in articles]
        }
        
        path = self.output / 'articles' / 'index.json'
        path.write_text(json.dumps(index, indent=2))
        return path
    
    def export_thread_index(self) -> Path:
        """Export active thread metadata as JSON."""
        threads = self.query_active_threads()
        
        index = {
            'generated_at': datetime.utcnow().isoformat(),
            'active_threads': [self.format_thread(t) for t in threads]
        }
        
        path = self.output / 'threads' / 'index.json'
        path.write_text(json.dumps(index, indent=2))
        return path
```

This is pseudocode. The actual implementation will adapt to Sieve's real schema once connected. The structure is intentionally simple — no ORM, no framework, just sqlite3, pathlib, json, and yaml.

## Dependencies

```
python >= 3.10
pyyaml
```

That's it. No framework. No build tools. Two imports beyond the standard library.
